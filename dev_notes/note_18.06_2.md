[](...menustart)

- [18.Determinants](#ae8178b3d3c78ee97adfcc298fe0b11e)
    - [Determinants , det A = |A|](#fc7bcc02eaa53ccf05c2b3f48461de47)
    - [Signed](#71fed0c3428bf1a2e19af257c4bac379)
    - [Properties](#9fc2d28c05ed9eb1d75ba4465abf15a9)
- [19. Formular for Determinant](#077014dde3a16154d48a5807d1c351bb)
    - [Formula for detA ( n! terms )](#a4ea43d88bcd4db4f7d7fe3abe7052b4)
    - [Cofactor formula](#d02910574d79a0726db429910342d33e)
- [20. APPLICATIONS OF DETERMINANTS](#d32bea3e6901eaca807e7ff0a03afc52)
    - [Formula for A‚Åª¬π](#23d31b234b8839bc2675b16fff1de2da)
    - [Cramers Rule for x=A‚Åª¬πb](#8d1f376d9179ba4326b17606a2f64136)
    - [| detA | = volume of box](#b99ff4a9543c3ccbfac05190cc07dd68)
- [21. EigenValues - EigenVectors](#2a051e916e56ff2ad1a6b47745f85a6b)
    - [det\[A-ŒªI\] = 0](#9e08a229f7feafd17be0f57fed7d544d)
    - [Trace = Œª‚ÇÅ+Œª‚ÇÇ+ ... + Œª<sub>n</sub>](#95946abeb88b298e53a6c970166fb7aa)
- [22. Diagonalizing](#7082803699d165541950b7de2ae708e3)
    - [Diagonalizing a matrix](#7b86df847bba7602c3c3d17d03b2841f)
    - [Powers of A / equation u<sub>k+1</sub> = Au<sub>k</sub>](#d975eb14bcadcace9392fa08591c6162)
    - [Recap](#8912c5512db9003e5c8ce07b7ff36a88)
- [23.](#d4fbca7834560377a100c91364ec53ec)
    - [Differential Equations  du/dt=Au](#9c0f59a904be7c01984a683e511ca015)
    - [Exponential e<sup>At</sup> of a matrix](#627bb884755ff95a306e6587c9bed913)
- [24.](#21250cdbabe0f7964564376f3d523f38)
    - [Markov matrices](#0134f701c2230c4f2fc4488d2214160a)
    - [Fourier Series and projections](#1c401566682351198dc0b2cf460094c1)
- [25.](#13f3022d707d2db79b2bbc152c21c483)
    - [Symmetric Matrices](#b899f85c23e42aea33f7684a076389ca)
    - [26.](#dd3391b01e6d23199e9d35f0a4f5d427)
        - [Complexo](#023666df9ae9590deecc67a35fb5839e)
        - [Complex Vectors , length](#e5c0693de0b5a3e017ca1bcabf2a8444)
        - [Complex Matrices](#9ff43aa3e54f31207991c2e82ccf292d)
    - [27](#02e74f10e0327ad868d138f2b4fdd6f0)
        - [Positive Definite Matrix (Tests)¬≤](#f374bd8e0ba50a2cca911b7d328d59fb)
        - [Ellipsoids in R‚Åø](#e77fe7259561aabc1f580cd55ad18daa)
    - [28](#33e75ff09dd601bbe69f351039152189)
        - [A·µÄA is positive definite](#714c646ab1eb70195e6ae70fbf2681b6)
        - [Similar Matrices A,B](#ae75137209c482f86ac4dd1100e7e6af)
    - [29](#6ea9ab1baa0efb9e19094440c317e21b)
        - [Singular Value Decomposition = SVD](#cdbd45bcf4e7ece0654c0d7b925abb97)
    - [30](#34173cb38f07f89ddbebc2ac9128303f)
        - [Linear Transformation](#1b08f35b032e4fa969a0b2e5ebcb2c03)
    - [31](#c16a5320fa475530d9583c34fd356ef5)
    - [33](#182be0c5cdcd5072bb1864cdee4d3d6e)
        - [Left/Right Inverse](#3f163d64e05a5e233e942266c95c8272)
        - [Pseudo Inverse](#3cb290203c57a167d5b432a708ce74dc)

[](...menuend)


<h2 id="ae8178b3d3c78ee97adfcc298fe0b11e"></h2>

# 18.Determinants 

Up to now we paid a lot of attention to rectangular matrices. 

Now, concentrating on square matrices. Determinants and Eigen values are big, big chunk of 18.06.

<h2 id="fc7bcc02eaa53ccf05c2b3f48461de47"></h2>

## Determinants , det A = |A| 

Every square matrix has a number associated with , called its determinant. 

- det A = 0 , means A is singular.  
- det A !=0 , means A is invertible.

<h2 id="71fed0c3428bf1a2e19af257c4bac379"></h2>

## Signed

Determinant is signed.

<h2 id="9fc2d28c05ed9eb1d75ba4465abf15a9"></h2>

## Properties 

3 base properties:

1. det I = 1.
2. Exchanging rows reverse sign of det.  
3. The determinant depends linearly on **one** row. (single row linearity)
    - 3a: Add vectors in a row
        - 
        ```
        |a+a' b+b'| =|a b| + |a' b'|
        |c    d   |  |c d|   |c  d |
        ```
    
    - 3b: Multiply by t in row
        - 
        ```
        |ka kb| = k |a b|
        | c  d|     |c d|
        ```
        - det2A = 2‚Åø detA
        - det(A+B) ‚â† detA + detB

---

<details>
<summary>
p4 ~ p10
</summary>

- p4: property 2 also said , dup rows => det=0 
- p5: elimination ( - k rowi from rowj ) doesn't change det.
    - Êï∞Â≠¶‰∏äÔºåÂáèÂéªÁöÑËøôÈÉ®ÂàÜ ‰ΩøÁî®p3 ÂèØ‰ª•ÂàÜÁ¶ªÂá∫ÂéªÔºåËøôÈÉ®ÂàÜÁöÑdet ‰∏∫0.
    - ÈõÜÂêà‰∏äÔºåÂπ≥Ë°åÂõõËæπÂΩ¢,Â∫ï‰∏çÂèòÂèëÁîüÂàáÂèòÔºåÈù¢ÁßØ‰∏çÂèò
- p6: zero row => det=0.
    - p3b, in case a=0,b=0 , K¬∑det=det => det=0.
- p7: triangular matrix, det = product of pivots
- p8: If A is singular, then det A = 0. If A is invertible, then det A ‚â† 0.
- p9: det AB = detA * detB  (very valuable property)
    - detA‚Åª¬π
- p10: detA·µÄ = detA 
    - Ë°åÂàóÂºèÔºåÊâÄÊúâË°åÁöÑÊÄßË¥®ÔºåÂØπÂàóÂêåÊ†∑ÊúâÊïà

</details>



<h2 id="077014dde3a16154d48a5807d1c351bb"></h2>

# 19. Formular for Determinant

<h2 id="a4ea43d88bcd4db4f7d7fe3abe7052b4"></h2>

## Formula for detA ( n! terms )

- use property 3 to break down each row 
    - [a b c] = [a 0 0] + [0 b 0] + [0 0 c]
- the determinant of A can be expanded into n‚Åø terms 
- a lot terms has 0 det,  they can be removed.
- The nonzero terms have to come in different columns and rows. 
    - ![](../imgs/LA_det_3x3_6terms.png)
- ![](../imgs/LA_det_bigFormula.png)
 
<h2 id="d02910574d79a0726db429910342d33e"></h2>

## Cofactor formula

Cofactor is a way of breaking up above big formula that connects the nxn determinant to a determinant one smaller. 

![](../imgs/LA_det_cofactor_filter.png)

- **The determinant of A is a combination of any row i times its cofactors**.
    - **det A by cofactors:** det A = a·µ¢‚ÇÅC·µ¢‚ÇÅ + a·µ¢‚ÇÇC·µ¢‚ÇÇ + ... + a·µ¢<sub>ùëõ</sub>C·µ¢<sub>ùëõ</sub>. (10)
    - The cofactor is the determinant of M·µ¢‚±º , with the correct sign:
        - **delete row i and column j:** C·µ¢‚±º = (-1)‚Å±‚Å∫ ≤ detM·µ¢‚±º.   (11)


<h2 id="d32bea3e6901eaca807e7ff0a03afc52"></h2>

# 20. APPLICATIONS OF DETERMINANTS

<h2 id="23d31b234b8839bc2675b16fff1de2da"></h2>

## Formula for A‚Åª¬π

![](../imgs/la_det_ap_inverse.gif)

<details>
<summary>
AC·µÄ = (detA) I
</summary>

![](../imgs/LA_det_apply_inverse3.png)

The critical question is: Why do we get zeros off the diagonal? 

The answer is: we are actually computing the determinant of a new matrix which has equal rows. 

</details>

<h2 id="8d1f376d9179ba4326b17606a2f64136"></h2>

## Cramers Rule for x=A‚Åª¬πb

![](../imgs/la_1806_cramer_rule.gif)

What do I get in C·µÄb ? What's the first entry of C·µÄb ?  

Somehow I multiply cofactors by the entries of b, anytime I'm multiplying cofactors by numbers, I think, I'm getting the determinant of something. Let's call the matrix B , whose determinant comes out of C·µÄb .

So x‚ÇÅ = detB‚ÇÅ/detA  , x‚ÇÇ = detB‚ÇÇ/detA , ... 

**4C:** Cramer's rule: The jth component of x = A‚Åª¬πb is the ratio

![](../imgs/LA_det_cramer_rule.png)

Actually, Cramer's Rule is a disastrous way to go, because compute these determinants takes like approximately forever.  

But having a formula allows you to do algebra instead of algorithms. They're nice formulas, but I just don't want you to use them. 


<h2 id="b99ff4a9543c3ccbfac05190cc07dd68"></h2>

## | detA | = volume of box


```
    Â∑≤Áü•‰∏âËßíÂΩ¢ÁöÑ‰∏â‰∏™È°∂ÁÇπÔºö
    (x1,y1),(x2,y2),(x3,y3),Ê±ÇÈù¢ÁßØ:
    Ëß£Ôºö
                |x1 y1 1|
    S= 1/2 det  |x2 y2 1|
                |x3 y3 1|
    
    
    Â¶ÇÊûúÊúâ‰∏™È°∂ÁÇπÊòØÂéüÁÇπ,ÊØîÂ¶Ç(x1,y1)=(0,0)
    
    S =  1/2 det|x2,y2|
                |x3,y3|
```

---

<h2 id="2a051e916e56ff2ad1a6b47745f85a6b"></h2>

# 21. EigenValues - EigenVectors 

There are certain vectors where Ax comes out parallel to x. And those are the eigenvectors.  **Ax=Œªx**.

If A is singular, Œª=0 is an eigenvalue.

Now the question is how do we find these x-s and Œª.

<details>
<summary>
Projection Matrix
</summary>

Any x in projection plane  Ax=x , Œª=1

Any x ‚üÇ plane,  Ax=0 , Œª=0.

</details>

<details>
<summary>
Permutation Matrix
</summary>

```
A =

   0   1
   1   0
```

x = [1;1] , Œª=1

x = [-1;1] , Œª=-1. In fact, the trace tells you right away what the other eigenvalue is.

</details>

How to solve Ax=Œªx ?

(A-IŒª)x = 0.    A-IŒª has to be singular.


<h2 id="9e08a229f7feafd17be0f57fed7d544d"></h2>

## det[A-ŒªI] = 0 

The idea will be to find Œª first. I'll find n Œª's.  A Œª could be repeated, A repeated Œª is source of all trouble in 18.06.

<h2 id="95946abeb88b298e53a6c970166fb7aa"></h2>

## Trace = Œª‚ÇÅ+Œª‚ÇÇ+ ... + Œª<sub>n</sub>

Fact: sum of Œª's = a‚ÇÅ‚ÇÅ+a‚ÇÇ‚ÇÇ+ ... + a<sub>nn</sub>.


<details>
<summary>
Symmetric Matrix Example 
</summary>

```
A =
   3   1
   1   3


det(A-ŒªI) = |3-Œª 1| = (3-Œª)¬≤-1
            |1 3-Œª|

          = Œª¬≤-6Œª+8 
```

This example is great. 6 is the trace ,and 8 is the determinant. 

Œª‚ÇÅ=4, Œª‚ÇÇ=2 . 

```
A-4I = 
  -1   1
   1  -1
```

x‚ÇÅ= [1;1]

```
A-2I = 
   1   1
   1   1
```

x‚ÇÇ= [-1;1]

</details>

What's the relateion between the Permutation example and Symmetric example ?

Simple but useful observation: if I add *n*¬∑I to a matrix, its eigenvectors don't change, and its eigen values are *n* bigger.


<details>
<summary>
Why does it happen ? 
</summary>

Suppose I have a matrix A, and Ax=Œªx. I add 3I to that matrix. 

(A+3I)x = Œªx + 3x = (Œª+3)x

</details>


<details>
<summary>
Rotation matrix Q
</summary>

```
Q = 
   0  -1
   1   0
```

90¬∞ rotation.

trace = 0 = Œª‚ÇÅ+Œª‚ÇÇ , det = 1 = Œª‚ÇÅŒª‚ÇÇ.  Œª¬≤=-1.

What I'm leading up to with this example is that something is gonna go wrong, because what vector can come out parallel to itself after a 90¬∞ rotation? 

Œª‚ÇÅ=i, Œª‚ÇÇ=-i. 

</details>

If a matrix was symmetric, it wouldn't happen ( complex number Œª ). So if we stick to matrices that are symmetric , or close to symmetri, then the eigenvalues will stay real. But if we move far away from symmetric , for this example it's anti-symmetric , the bad thing will happen. 


<details>
<summary>
Here's one more bad thing that could happen.
</summary>

```
A =
   3   1
   0   3
```

This is a triangular matrix.  It's really useful to know you can read the eigenvalues off, they're  right on the diagonal. 

Œª‚ÇÅ = Œª‚ÇÇ = 3. 

```
(A-ŒªI)x = |0 1|x = 0
          |0 0|
```

x‚ÇÅ=[1;0]  , x‚ÇÇ= NO 2nd independent x 

This is a degenerate matrix. It's only got one line of eigenvectors instead of two.  It's this possibility of a repeated eigenvalue opens this further possiblity of a shortage of eigenvectors. 

</details>


<h2 id="7082803699d165541950b7de2ae708e3"></h2>

# 22. Diagonalizing 

<h2 id="7b86df847bba7602c3c3d17d03b2841f"></h2>

## Diagonalizing a matrix 


<details>
<summary>
S‚Åª¬πAS = Œõ
</summary>

Suppose we have n independent eigenvectors of A , put them in columns of S.  
S is the eigenvector matrix. 

```
     ‚é° |  |      |  ‚é§   ‚é° |    |        |   ‚é§
AS = ‚é¢ x‚ÇÅ x‚ÇÇ ... xn ‚é• = ‚é¢Œª‚ÇÅx‚ÇÅ Œª‚ÇÇx‚ÇÇ ... Œªnxn ‚é•.
     ‚é¢ |  |      |  ‚é•   ‚é¢ |    |        |   ‚é•
     ‚é£ |  |      |  ‚é¶   ‚é£ |    |        |   ‚é¶
```

Each time A multiply eigenvectors , the eigen value comes out.

```
     ‚é°  |   |        |  ‚é§   ‚é°  | |      |  ‚é§ ‚é°Œª‚ÇÅ       ‚é§
AS = ‚é¢Œª‚ÇÅx‚ÇÅ Œª‚ÇÇx‚ÇÇ ... Œªnxn‚é• = ‚é¢ x‚ÇÅ x‚ÇÇ ... xn ‚é• ‚é¢  Œª‚ÇÇ     ‚é•.
     ‚é¢  |   |        |  ‚é•   ‚é¢  | |      |  ‚é• ‚é¢    ...  ‚é•
     ‚é£  |   |        |  ‚é¶   ‚é£  | |      |  ‚é¶ ‚é£       Œªn‚é¶
```

If I want a number to multiply x,  then I can do it by puting x·µ¢ in column i (I get S back again), and multiply by a diagonal matrix. 

**AS = SŒõ**. 

Provided my assumption of n independent eigenvectors,  I multiply S‚Åª¬π on the left , I got 

**S‚Åª¬πAS=Œõ**.

if I multiply  S‚Åª¬π on the right , I got 

**A=SŒõS‚Åª¬π**.

</details>


<h2 id="d975eb14bcadcace9392fa08591c6162"></h2>

## Powers of A / equation u<sub>k+1</sub> = Au<sub>k</sub>

<details>
<summary>
Can I just begin to use that ? For example how about A¬≤ ? What's eigenvectors and eigenvalues of A¬≤ ? 
</summary>

If Ax = Œªx 

A¬≤x = ŒªAx = Œª¬≤x. 

So the eigenvalues of A¬≤ are Œª¬≤, and the eigenvector is the same x as A.

This is one way to do it . 

Now let me see that also from formula A=SŒõS‚Åª¬π.

A¬≤x = SŒõS‚Åª¬π¬∑SŒõS‚Åª¬πx = SŒõ¬≤S‚Åª¬π. It's telling me the same thing that I just learned here, but in a matrix form. It's telling me that the S is the same, the eigenvectors are the same; but the eigenvalues are squared.

</details>

A·µè = SŒõ·µèS‚Åª¬π . 

Eigenvalues and eigenvectors give a great way to understand the powers of a matrix. 

Theorem: A·µè‚Üí0 as k‚Üí‚àû  if all |Œª·µ¢|<1 .  

So far I'm operationg on one assumption: I had a full set of n independent eigenvectors. If we don't have n independent eigenvectors can not diagonalize the matrix,  S‚Åª¬π can not make sense.

**A is sure to have n independent eigenvectors (and be diagonalizable) if all the Œª's are different (no repeated Œª)**. 

If I have repeated Œª's , I may or may not have n independent eigenvectors (think about the Identity matrix). 


<details>
<summary>
Start with given vector u‚ÇÄ , u<sub>k+1</sub> = Au<sub>k</sub>, calculate u‚ÇÅ‚ÇÄ‚ÇÄ
</summary>

u‚ÇÅ=Au‚ÇÄ, u<sub>k</sub>=A·µèu‚ÇÄ

The next section is gonna to solve systems of differential equations. I'm going to have derivatives. 

write u‚ÇÄ as the combination of eigenvectors: 

u‚ÇÄ = c‚ÇÅx‚ÇÅ +c‚ÇÇx‚ÇÇ + ... +c<sub>n</sub>x<sub>n</sub>

Now multiply by A :

Au‚ÇÄ = c‚ÇÅŒª‚ÇÅx‚ÇÅ +c‚ÇÇŒª‚ÇÇx‚ÇÇ + ... +c<sub>n</sub>Œª<sub>n</sub>x<sub>n</sub>

u‚ÇÅ‚ÇÄ‚ÇÄ = A¬π‚Å∞‚Å∞u‚ÇÄ = c‚ÇÅŒª‚ÇÅ¬π‚Å∞‚Å∞x‚ÇÅ +c‚ÇÇŒª‚ÇÇ¬π‚Å∞‚Å∞x‚ÇÇ + ... +c<sub>n</sub>Œª<sub>n</sub>¬π‚Å∞‚Å∞x<sub>n</sub> = Œõ¬π‚Å∞‚Å∞Sc


</details>


<details>
<summary>
Fibonacci Example:  F<sub>k+2</sub> = F<sub>k+1</sub> + F<sub>k</sub>
</summary>

0,1,1,2,3,5,8,13, ... 

F<sub>k+2</sub> = F<sub>k+1</sub> + F<sub>k</sub>

Right now what I've got is a single equation, not a system, and it's second-order, with second derivatives. I want to get first derivatives. The way to do it is to introduce u<sub>k</sub> will be a vector:

![](../imgs/la_1806_uk.png)

This is my unknown. 

So I'm going to get a 2x2 system, first order, instead of a scalar system, second order:

![](../imgs/la_1806_fib_system.png)

This is my system. 

And what's my one step equation? 

![](../imgs/la_1806_fib_equation.png)

What's the eigenvalues and eigenvectors of this matrix ?  

Œª‚ÇÅ = -0.61803, Œª‚ÇÇ = 1.61803 . 

How fast are those Fibonacci numbers increasing ? What's controlling the growth of these Fibonacci numbers? It's the eigenvalues. 

And which eigenvalue is controlling that growth? The big one. 

Since u‚ÇÄ = [1;0] , c‚ÇÅx‚ÇÅ+c‚ÇÇx‚ÇÇ = [1;0].

F‚ÇÅ‚ÇÄ‚ÇÄ ‚âà c‚ÇÅ¬∑(1.61803)¬π‚Å∞‚Å∞.   ( the other terms involves c‚ÇÇ is ignored since it is very slow. )



</details>

<h2 id="8912c5512db9003e5c8ce07b7ff36a88"></h2>

## Recap 

- eigenvalue Œª
    1. nxn matrix has n eigenvalues
    2. eigenvalue is allowed to be 0 ( i.e. projection matrix )
    3. sum of Œª's  equals the trace of matrix,  
    4. product of Œª's equals determinant of matrix. 
    5. singular matrix must have 0 Œª.  
        - number of 0 Œª's = dimension of null space.
    6. repeated Œª **MAY or may NOT** result in missing eigenvectors
    7. eigenvalue could be complex number.
        - Symmetric matrix's eigenvalues are all real. 
- eigenvector 
    1. Eigenvectors of sysmmetric matrix are orthogonal.  (doesn't all eigenvectors orthogonal )
    2. A matrix may have no eigenvectors.  ( ? any example ? )


<h2 id="d4fbca7834560377a100c91364ec53ec"></h2>

# 23. 

This section is about how to solve a system of first order, first derivative, constant coefficient linear equations. And if we do it right, it turns directly into linear algebra. The key idea is the sulutions to contant coefficient linear equations are **exponentials**.  

And the result, one thing we will find it's completely parallel to powers of a matrix. 


<h2 id="9c0f59a904be7c01984a683e511ca015"></h2>

## Differential Equations  du/dt=Au

<details>
<summary>
Example 
</summary>

u(0) = [1;0]

du‚ÇÅ/dt = -u‚ÇÅ+2u‚ÇÇ

du‚ÇÇ/dt =  u‚ÇÅ-2u‚ÇÇ

-> A = ‚é° -1  2 ‚é§
       ‚é£  1 -2 ‚é¶

So it starts u at time 0, everything starts at u‚ÇÅ. And as time goes on, du‚ÇÇ/dt weill be positive because of that u‚ÇÅ term, so flow will move into the u‚ÇÇ component and it will go out of the u‚ÇÅ component. 

So we'll just follow that movement as time goes forward by looking at the eigenvalues and eigenvectors of that matrix.

Œª‚ÇÅ = 0, Œª‚ÇÇ = -3.

The eigen values are telling me something. Œª‚ÇÇ = -3 , this eigen value is going to disappear, e‚Åª¬≥·µó.  Œª‚ÇÅ = 0, e‚Å∞·µó=1. 

So I'm expecting that this solution'll have 2 parts, e‚Å∞·µó and e‚Åª¬≥·µó parts. and as time goes on, the second part'll disappear and the first part will be a steady status. ( Âõ†‰∏∫ËøôÊòØÂæÆÂàÜÊñπÁ®ãÔºåÊâÄ‰ª•0ÊòØsteady status??? )

x‚ÇÅ=[2;1] , x‚ÇÇ=[1;-1]

Solution: u(t) = c‚ÇÅe<sup>Œª‚ÇÅ</sup>x‚ÇÅ + c‚ÇÇe<sup>Œª‚ÇÇ</sup>x‚ÇÇ

TODO Áúã‰π¶„ÄÇ




</details>


<h2 id="627bb884755ff95a306e6587c9bed913"></h2>

## Exponential e<sup>At</sup> of a matrix

TODO

---

<h2 id="21250cdbabe0f7964564376f3d523f38"></h2>

# 24. 

<h2 id="0134f701c2230c4f2fc4488d2214160a"></h2>

## Markov matrices


```bash
A =

   0.10000   0.01000   0.30000
   0.20000   0.99000   0.30000
   0.70000   0.00000   0.40000
```

<details>
<summary>
property: 
</summary>

1. all entreis >= 0
2. all columns add to 1.

The powers of Markov matrix are all Markov matrices.

**Steady State**:  Œª = 1. 

Key points:

1.  Œª = 1 is an eigenvalue
2. all other |Œª·µ¢| < 1

Remember: uk = A·µèu‚ÇÄ = c‚ÇÅŒª‚ÇÅ·µèx‚ÇÅ +c‚ÇÇŒª‚ÇÇ·µèx‚ÇÇ + ... +cnŒªn·µèxn 

Those terms which has |Œª·µ¢| < 1 goes to 0.

since all columns of A markov matrix add to 1, then A-I must be singular ( all columns of A-I add to 0 ) . That is , one of the eigenvalues must be 1. 

**Eigenvalus of A·µÄ are the same as eigenvalues of A.**  because det(A-ŒªI) = det(A·µÄ-ŒªI).  (Determinant's property 10.)

</details>

<details>
<summary>
application :
</summary>

u<sub>k+1</sub> = Au<sub>k</sub> , A is Markov Matrix

The populations in California and Massachusetts.

Every year, 90% people of California stays, and 10% moves to Massachusetts, while 80% people of Massachusetts stays , and 20% moves to California.

```
u = |u_cal |
    |u_mass|

A =

   0.90000   0.20000
   0.10000   0.80000
```

after many many  yeas later, where are those peoples ? 

We can easily get the 2 eigenvalues : Œª‚ÇÅ=1, Œª‚ÇÇ=0.7.

and the eigenvector for the steady status is x‚ÇÅ=[2;1].  Now we can jump to infinity right now, finally , the California will have 2/3 total peoples, while Mass has 1/3 of all. 

To verify: 

let u‚ÇÄ = [0;1000],

since u‚ÇÄ = c‚ÇÅx‚ÇÅ +c‚ÇÇx‚ÇÇ  =  c‚ÇÅ[2;1] +c‚ÇÇ[-1;1] , we get c‚ÇÅ = 1000/3.

A<sup>k+1</sup>u‚ÇÄ = c‚ÇÅ¬∑1<sup>k+1</sup>[2;1] = [666.67 ; 333.33] 



</details>



<h2 id="1c401566682351198dc0b2cf460094c1"></h2>

## Fourier Series and projections 


<details>
<summary>
Projections with orthonormal basis  
</summary>

for any v ,  

v = x‚ÇÅq‚ÇÅ+ x‚ÇÇq‚ÇÇ+ ... + x<sub>n</sub>q<sub>n</sub>

how to calculate x‚ÇÅ quickly ?  multiplied by q‚ÇÅ·µÄ on both side 

q‚ÇÅ·µÄv = x‚ÇÅ.

This result can be achieved from the maxtix view .

Qx = v.  x=Q‚Åª¬πv = Q·µÄv  => x‚ÇÅ=q‚ÇÅ·µÄv


</details>

<details>
<summary>
Fourier Series 
</summary>

f(x) =a‚ÇÄ + a‚ÇÅcosx + b‚ÇÅsinx + a2cos2x + b‚ÇÇcos2x + ... 

This one's infinite, but the key property of things being orthogonal is still true for sin and cos. So it is the propery that makes Fourier series work. 

Joseph Fourier realized that, hey, I could could work in function space.

Instead of a vector v, I could have a function f(x) . Instead of orthornal vectors q‚ÇÅ,q‚ÇÇ,..., I could have orthogonal functions, but infinitely many of them. 

So the basis are functions , 1, cosx, sinx, cos2x, sin2x, ... , and they are orthogonal. 

What does it means "funtions are orthogonal" ?

f·µÄg = ‚à´‚ÇÄ<sup>2œÄ</sup> f(x)g(x)dx = 0.

How do I get a‚ÇÅ?  I take the inner product of everything with cos(x).


</details>


---

<h2 id="13f3022d707d2db79b2bbc152c21c483"></h2>

# 25.

<h2 id="b899f85c23e42aea33f7684a076389ca"></h2>

## Symmetric Matrices

A = A·µÄ
 
<details>
<summary>
Eignvalues / Eigenvectors 
</summary>

What's special about the eigenvectors ?

1. The eigenvalues are REAL.
2. The eigenvectors are (can be chosen) PERPENDICULAR

Usual case :  A = SŒõS‚Åª¬π
Symmetric case: A = QŒõQ‚Åª¬π = QŒõQ·µÄ , one of the most famous theorem.

Why real eigenvalues ?

TODO

If A is a real matrix , A has real eigenvalues and perpendicular eigenvectors means  A = A·µÄ.

If A is a complex matrix ,  it meas A = AÃÖ·µÄ.

A = QŒõQ·µÄ  = Œª‚ÇÅq‚ÇÅq‚ÇÅ·µÄ + Œª‚ÇÇq‚ÇÇq‚ÇÇ·µÄ + ...  

here all q are orthonoraml , and qq·µÄ is a projection matrix.

Every symmetric matrix is a combination of perpendicular projection matrices. 

Symmetric  matrix has another property:  #positive pivots == #positive eigenvalues.

</details>


<details>
<summary>
Start: Positive Definite Matrices
</summary>

It's symmtrix.   

1. All eigenvalues are positive.  
2. All the pivots are positive.  
3. **all** the **sub**determinant is positive. (including the determinant) 

</details>

<h2 id="dd3391b01e6d23199e9d35f0a4f5d427"></h2>

## 26.

<h2 id="023666df9ae9590deecc67a35fb5839e"></h2>

### Complexo

The Fourier Matrix is the most important complex matrix.  It's the matrix that we need in Fourier transform.  

And the really special thing is the fast Fourier transform (FFT). It's being used in a thousand places. How do I mulitply fast by that matrix? 

Normally, multiplications by nxn matrix would be O(n¬≤).  The FFT's idea reduces this n¬≤ down to nlog‚ÇÇn. 

<h2 id="e5c0693de0b5a3e017ca1bcabf2a8444"></h2>

### Complex Vectors , length

z·µÄz is no good.  Doing that inner productio won't give me the right thing.

Because the length square should be positive, but complex number may make it 0 or negative.

What I really want is zÃÖ·µÄz = |z|¬≤. 

Here is a symbol to do both: z·¥¥z.   H stands for Hermite. 

**Inner product** of complex vector:  yÃÖ·µÄx.


<h2 id="9ff43aa3e54f31207991c2e82ccf292d"></h2>

### Complex Matrices

Symmetric A·µÄ=A  is no good if A is complex. 

right version:  AÃÖ·µÄ=A.   The diagonal should be real, and the other entries should be conjugate.  Hermitian matrix :  A·¥¥=A

```octave
A =
   2        3 + 1i
   3 - 1i   5 
```

Unitary matrix: like an orthogonal matrix, nxn matrix with orthonormal columns, perpendicular unit columns, perpendicularity is computed by **conjugate**. Fourier matrix happens to be one of these guys. 

1. the 1st column fills with 1
2. the 2nd column is 1, w, w¬≤, ... w‚Åø‚Åª¬π
3. the 3rd column is the power of 2nd column: 1,w¬≤,w‚Å¥,...,w¬≤‚ÅΩ‚Åø‚Åª¬π‚Åæ
4. more columns ...
5. the last colun is 1,w‚Åø‚Åª¬π, w¬≤‚ÅΩ‚Åø‚Åª¬π‚Åæ, ..., w‚ÅΩ‚Åø‚Åª¬π‚Åæ‚ÅΩ‚Åø‚Åª¬π‚Åæ


This is a symmetric matrix , not in perfect way, and 

(F<sub>n</sub>)<sub>ij</sub> = w‚Å± ≤ ,  i,j=0,...,n-1

Here, w is a special number.

w‚Åø=1.

w = e<sup>i2œÄ/n</sup> = ~~cos 2œÄ/n + i¬∑sin 2œÄ/n~~

w is no the unit circle of complex plane. for example ,if n = 6, then w lands on unit circle ( 1/6 circle, 60 degree ). 

Where is w¬≤ ? the angle is doubled ( 2/6 circle, 120 degree )

if n =4 , w<sub>n</sub> = e<sup>iœÄ/2</sup> = i .

```octave
F‚ÇÑ = 

  1  1  1  1
  1  i  i¬≤ i¬≥
  1  i¬≤ i‚Å¥ i‚Å∂
  1  1¬≥ i‚Å∂ i‚Åπ

   = 

  1  1  1  1
  1  i -1 -i
  1 -1  1 -1
  1 -i -1  i
```

This matrix is so remarkable.   It's the 4x4 matrix that comes into the 4 point Fourier transform. The inverse matrix will be a nice matrix also. 

Those columns are orthogonal ( PS. you should take 1 column's conjugate ). They're not quite orthonormal.  But I can fix it easily, they all have length 2. 

```octave
F‚ÇÑ = 1/2 *

  1  1  1  1
  1  i -1 -i
  1 -1  1 -1
  1 -i -1  i
```

Now I can inverse it right away:  F‚ÇÑ·¥¥.

So what's good? What property is it that leads to the FFT ? 

Here is the idea. F‚ÇÜ has a neat connection to F‚ÇÉ, half as big. There's a connection of F‚Çà to F‚ÇÑ. There's a connection of F‚ÇÜ‚ÇÑ to F‚ÇÉ‚ÇÇ. 

(w‚ÇÜ‚ÇÑ)¬≤ = w‚ÇÉ‚ÇÇ.

![](../imgs/la_fourier_6to3.png)

F‚ÇÜ‚ÇÑ ‰πüÊúâÁ±ª‰ºº‰∏äÈù¢ÁöÑ F‚ÇÜ=DF‚ÇÉP ÁöÑÂàÜËß£ÔºåËøôÊ†∑Â∞±ÂèØ‰ª• 64¬≤ Èôç‰Ωé‰∏∫ 2(32)¬≤+32.

D is powers of w 

```octave
D = 

  1  
     w
       w¬≤
         ...
            w‚Åø‚Åª¬π
```

Q: what is the P ?  How does recursive idea use here ? 

64¬≤ -> 2(32)¬≤+32 -> 2( 2(16)¬≤+16 ) + 32

When I do the recursion more and more times, I get simpler and simpler factors in the middle, eventually I'll be down to 2 point or 1 point Fourier transforms.  Finally I get to 6x32, that's my final count. 

Conclusion: the FFT multiplies by nxn matrix, but it does it not in n¬≤ steps, but in 1/2¬∑nlogn steps. 


<h2 id="02e74f10e0327ad868d138f2b4fdd6f0"></h2>

## 27 

<h2 id="f374bd8e0ba50a2cca911b7d328d59fb"></h2>

### Positive Definite Matrix (Tests)¬≤

Now, all matrices are symmetric. my question is , is it postive definite.

1. all eigen values are positive
2. all determinate of submatrices are postive.
3. all pivots are positive. 
4. `*`  x·µÄAx > 0

Graphics of f(x,y) = x‚Éó·µÄAx‚Éó = ax¬≤+2bxy+cy¬≤ 

saddle point, imagine the graph something like human legs.


<h2 id="e77fe7259561aabc1f580cd55ad18daa"></h2>

### Ellipsoids in R‚Åø

A = QŒõQ·µÄ  (how to connect them ? )




<h2 id="33e75ff09dd601bbe69f351039152189"></h2>

## 28 


<h2 id="714c646ab1eb70195e6ae70fbf2681b6"></h2>

### A·µÄA is positive definite

If A,B are pos def , so is A+B.  x·µÄ(A+B)x > 0.

x·µÄA·µÄAx = (Ax)·µÄAx = |Ax|¬≤ >= 0.

One comment about numerial things: with a positive definte matrix, you never have to do row exchanges, you never run into unsuitably small numbers or zeroes in the pivot postion. 


<h2 id="ae75137209c482f86ac4dd1100e7e6af"></h2>

### Similar Matrices A,B

nxn matrices A and B are simular means : for some invertible M, B = M‚Åª¬πAM.

Example:

S‚Åª¬πAS = Œõ  , A is similar to Œõ.

like, I'm putting these matrices into families, all the matrices in the family are similar to each other. The outstanding member of the family is the diagonal guy (i.e. Œõ) . 

suppose 

```octave
A =

   2   1
   1   2
```

```octave
Œõ =

   3   0 
   0   1 
```

```octave
M =

   1   4
   0   1


B = M‚Åª¬πAM = 

   -2  -15
    1    6
```

A,Œõ,B are all similar matrices. They all have something in common. And that is they have the same eigenvalues.  ( and same number of eigenvectors. )  

proof:

```octave
Ax=Œªx  (B=M‚Åª¬πAM)
AMM‚Åª¬πx = Œªx 
M‚Åª¬πAMM‚Åª¬πx = ŒªM‚Åª¬πx 
BM‚Åª¬πx = ŒªM‚Åª¬πx  , let b=M‚Åª¬πx
Bb = Œªb
```

Eigenvector of B is M‚Åª¬π(eigenvector of A).

---- 

I now have to get into the less happy possibility that the 2 eigenvalues could be the same. 

BAD CASE 

for example, Œª‚ÇÅ=Œª‚ÇÇ=4.

There are 2 families :

1. the small one family includes only 1 matrix : 4I 
    - 
    ```octave
    A =

       4   0
       0   4
    ```
    - since M‚Åª¬πAM = A 
2. the big one family includes 
    - 
    ```octave
    A =

       4   1
       0   4
    ```
    - those matrices has only 1 eigenvector.
    - more members of family
    ```octave
    B =

       5   1
      -1   3
    ```


<h2 id="6ea9ab1baa0efb9e19094440c317e21b"></h2>

## 29

<h2 id="cdbd45bcf4e7ece0654c0d7b925abb97"></h2>

### Singular Value Decomposition = SVD

A = UŒ£V·µÄ

- It is the final and best factorization of a matrix.
    - ‚àë diagonal matrix
    - U,V  orthogonal matrix

A can be any matrix.  Any matrix has this SVD. 

One thing we'll bring together is the very good family: 


- symmtric , postive , definte matrix.
    - A = QŒõQ·µÄ.  Because they were symmetic , their eigenvectors were orthogonal.
    - This is the SVD in case our matrix is symmetric postive definte.

- A = SŒõS·µÄ. 
    - This is my usual one. My usual one is the eigenvectors and eigenvalues. This is no good in general because usually S isn't orthogonal.
    - In the symmetric case , the eigenvectors are orthogonal, so my ordinary S has become Q. And 
    - In the positive definite case , the ordinary Œõ become positive Œõ.

---

What am I looking for now? 

Av‚ÇÅ=u‚ÇÅ.  v‚ÇÅ in row space gets taken over to u‚ÇÅ in column space.

In SVD, what I'm looking for is an orthogonal basis in row space that gets knocked over into an orthogonal space in columm space.

I'm not only going to make these orthogonal, but also make them orthonormal. 

œÉ‚ÇÅu‚ÇÅ=Av‚ÇÅ

œÉ‚ÇÇu‚ÇÇ=Av‚ÇÇ

AV = UŒ£

So, this is my goal, to find an orthonormal basis in the row space(V), and an orthonormal basis in the column space(U).

A = U‚àëV‚Åª¬π = U‚àëV·µÄ

Now I have 2 orthogonal matrices here, And I don't want to find them both at once. So I want to cook up some expression that will make the Us disappear and leave me only the V. 

It's the same combination that keeps showing up whenever we have a general rectangular matrix:

A·µÄA = V‚àë·µÄU·µÄU‚àëV·µÄ = V‚àë¬≤V·µÄ

Now Us are out of the picture now. I'm only having to choose the Vs, and what are these Vs?  And what are these ‚àës ?

They're the eigenvectors and eigenvalues for the matrix A·µÄA.

How I going to find the Us ? One way would be to look at AA·µÄ. 

So the overall picture is , the Vs are the eigenvectors of A·µÄA , and Us are the eigenvectors of AA·µÄ. The œÉs are the positive  square roots of eigenvalue of A·µÄA ?

Example

```octave
A =

   4   4
  -3   3

A·µÄA =

   25    7
    7   25
```

v‚ÇÅ = [1/‚àö2;1/‚àö2] ,  v‚ÇÇ = [1/‚àö2, -1/‚àö2]

œÉ‚ÇÅ = 32 , œÉ‚ÇÇ = 18

So 

```octave
V = 

    1/‚àö2    1/‚àö2 
    1/‚àö2   -1/‚àö2

‚àë =

    ‚àö32     0
     0     ‚àö18
```

```octave
AA·µÄ = 
   
   32    0
    0   18
```

u‚ÇÅ = [1;0] , u‚ÇÇ = [0;-1]

The eigenvalues are the same as A·µÄA.


---

<h2 id="34173cb38f07f89ddbebc2ac9128303f"></h2>

## 30

<h2 id="1b08f35b032e4fa969a0b2e5ebcb2c03"></h2>

### Linear Transformation 

T(v+w) = T(v)+T(w)

T(cv) = cT(v)

In linear transformation, zero vector can not move.  T(0) = 0 .

Example: Matrix multiplication

T(v) = Av


This idea of a linear transformation is like kind of the abstract description of matrix multiplication. And what's our goal here?  Our goal is to understand linear transformations, and the way to understand them is to find a matrix that lies behind them.  That's really the idea. 

Information needed to know T(v) for all inputs:

If I know what the transformation does to every vector in a basis, then I know everything. 

Construct matrix A that represents linear Transformation T.

<h2 id="c16a5320fa475530d9583c34fd356ef5"></h2>

## 31 

JPEG , it changes the basis

standard basis:

[1;0;0...;0], [0;1;0...;0], [0;0;0...;1]

better basis:

[1;1;1,...;1] , very useful for solid image(i.e. a black image)

[1;1;1,...-1;-1;-1] , half dark , half light

[1;-1;1;-1;...1;-1;1;-1] ,  checkerboard

Those basis are all part of the 

Fourier basis (8x8 is better)

```
            signal
              |
   lossless   |  change basis
              ‚Üì  
          coeffs c
              |
    lossy     |  compress
              ‚Üì  
          coeffs cÃÇ (many zeros)
              |
              |
              ‚Üì
         xÃÇ = ‚àëcÃÇ·µ¢v·µ¢ 
```

What will happen at the compression step?

One thing we could do is just throw away the small coefficients. That is called thresholding.

Probably, there is enough of this vector of all 1s we very seldom throw that away. Usually,its coefficient will be large.

But the coefficient of something like the 'checkerboard' that quickly alternative vector, there's probably very little of that in any smooth signal. 

xÃÇ:  this sum doesn;t have 64 terms any more.   Probably it has about 2 or 3 terms. That's the kind of compression you are looking for. 


let's call the input signal `p` ,  what is the coefficients `c` ?  c = M‚Åª¬πp

This shows the critical point. A good basis has a nice,fast inverse.

- Properties of good basis
    1. fast   Fourier basis has FFT.
    2. Few is enough. 
        - A few basis vectors are enough to reproduce the image.

JPEG 2000 will include Wavelet basis.

---

ÁèçÂ¶ÆÂºó's story

---

The very best basis is the eigenvector basis. 

```
Œª‚ÇÅ 0        0
0  Œª‚ÇÇ       0
0  0        0
      ...
0  0        Œªn
```

So that's the perfect basis, that's the basis we'd love to have for image processing. 

But to find the eigenvectors of our pixel matrix would be too expensive. So we do something cheaper and close which is to choose a good basis like wavelets.


<h2 id="182be0c5cdcd5072bb1864cdee4d3d6e"></h2>

## 33

<h2 id="3f163d64e05a5e233e942266c95c8272"></h2>

### Left/Right Inverse

2-sided inverse : AA‚Åª¬π = I = A‚Åª¬πA


Left Inverse:  If A is full column rank(r=n) rectangle matix ,  (A·µÄA) is a good matrix, (A·µÄA)‚Åª¬πA·µÄ  is A's left inverse.

A rectangular matrix can't have a 2-sided inverse. 

If I multiply the left inverse on the wrong side  A(A·µÄA)‚Åª¬πA·µÄ , it is a projection matrix. It is the projection onto the column space. It's trying to be the identity matrix which is an impossible job. 

---

Right Inverse:  r=m , (AA·µÄ) is good matrix.   A·µÄ(AA·µÄ)‚Åª¬π is A's right inverse.

If I multiply the right inverse on the wrong side, A·µÄ(AA·µÄ)‚Åª¬πA is a projection matrix too, it is the projection on the row space.

<h2 id="3cb290203c57a167d5b432a708ce74dc"></h2>

### Pseudo Inverse

Ax = b  , row space, column space one to one mapping.

If x‚â†y both in row space, then Ax ‚â† Ay.

Proof: since x,y in row sapce,  x-y is also in row space. Suppose Ax=Ay , then A(x-y) = 0.  That is, x-y also in nullspace. x-y should be the zero vector, that is x=y. 

From the row space to the column space,  it we limited it to those 2 spaces , and then its inverse, called pseudo-inverse.  A matrix A is really a nice,invertible mapping from row space to columns space. 

Find the Pseudo Inverse A‚Å∫

- Start from SVD :  A=U‚àëV·µÄ 
    - ‚àë‚àë‚Å∫ , (mxm), trying to be I,  projection on column space
    - ‚àë‚Å∫‚àë , (nxn), trying to be I,  projection on row space
    - A‚Å∫ = V‚àë‚Å∫U·µÄ
























